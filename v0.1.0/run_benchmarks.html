<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Running a benchmark suite · PkgBenchmark.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>PkgBenchmark.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">Home</a></li><li><a class="toctext" href="define_benchmarks.html">Defining a benchmark suite</a></li><li class="current"><a class="toctext" href="run_benchmarks.html">Running a benchmark suite</a><ul class="internal"><li><a class="toctext" href="#More-advanced-customization-1">More advanced customization</a></li></ul></li><li><a class="toctext" href="comparing_commits.html">Comparing commits</a></li><li><a class="toctext" href="export_markdown.html">Export to markdown</a></li><li><a class="toctext" href="ref.html">Reference</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href="run_benchmarks.html">Running a benchmark suite</a></li></ul><a class="edit-page" href="https://github.com/JuliaCI/PkgBenchmark.jl/blob/master/docs/src/run_benchmarks.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Running a benchmark suite</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Running-a-benchmark-suite-1" href="#Running-a-benchmark-suite-1">Running a benchmark suite</a></h1><p>Use <code>benchmarkpkg</code> to run benchmarks defined in a suite as defined in the previous section.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="PkgBenchmark.benchmarkpkg" href="#PkgBenchmark.benchmarkpkg"><code>PkgBenchmark.benchmarkpkg</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">benchmarkpkg(pkg, [target]::Union{String, BenchmarkConfig}; kwargs...)</code></pre><p>Run a benchmark on the package <code>pkg</code> using the <a href="ref.html#PkgBenchmark.BenchmarkConfig"><code>BenchmarkConfig</code></a> or git identifier <code>target</code>. Examples of git identifiers are commit shas, branch names, or e.g. &quot;HEAD~1&quot;. Return a <a href="run_benchmarks.html#PkgBenchmark.BenchmarkResults"><code>BenchmarkResults</code></a>.</p><p>The argument <code>pkg</code> can be a name of a package or a path to a directory to a package.</p><p><strong>Keyword arguments</strong>:</p><ul><li><p><code>script</code> - The script with the benchmarks, if not given, defaults to <code>benchmark/benchmarks.jl</code> in the package folder.</p></li><li><p><code>resultfile</code> - If set, saves the output to <code>resultfile</code></p></li><li><p><code>retune</code> - Force a re-tune, saving the new tuning to the tune file.</p></li></ul><p>The result can be used by functions such as <a href="comparing_commits.html#BenchmarkTools.judge"><code>judge</code></a>. If you choose to, you can save the results manually using <a href="ref.html#PkgBenchmark.writeresults-Tuple{String,PkgBenchmark.BenchmarkResults}"><code>writeresults</code></a> where <code>results</code> is the return value of this function. It can be read back with <a href="ref.html#PkgBenchmark.readresults-Tuple{String}"><code>readresults</code></a>.</p><p>If a <code>REQUIRE</code> file exists in the same folder as <code>script</code>, load package requirements from that file before benchmarking.</p><p><strong>Example invocations:</strong></p><pre><code class="language-julia">using PkgBenchmark

benchmarkpkg(&quot;MyPkg&quot;) # run the benchmarks at the current state of the repository
benchmarkpkg(&quot;MyPkg&quot;, &quot;my-feature&quot;) # run the benchmarks for a particular branch/commit/tag
benchmarkpkg(&quot;MyPkg&quot;, &quot;my-feature&quot;; script=&quot;/home/me/mycustombenchmark.jl&quot;)
benchmarkpkg(&quot;MyPkg&quot;, BenchmarkConfig(id = &quot;my-feature&quot;,
                                      env = Dict(&quot;JULIA_NUM_THREADS&quot; =&gt; 4),
                                      juliacmd = `julia -O3`))</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaCI/PkgBenchmark.jl/blob/d3e7a0de30c5a1346eb827c512fa7e4111fcd825/src/runbenchmark.jl#L1-L33">source</a></section><p>The results of a benchmark is returned as a <code>BenchmarkResult</code></p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="PkgBenchmark.BenchmarkResults" href="#PkgBenchmark.BenchmarkResults"><code>PkgBenchmark.BenchmarkResults</code></a> — <span class="docstring-category">Type</span>.</div><div><p>Stores the results from running the benchmarks on a package.</p><p>The following (unexported) methods are defined on a <code>BenchmarkResults</code> (written below as <code>results</code>):</p><ul><li><p><code>name(results)::String</code> - The commit of the package benchmarked</p></li><li><p><code>commit(results)::String</code> - The commit of the package benchmarked. If the package repository was dirty, the string <code>&quot;dirty&quot;</code> is returned.</p></li><li><p><code>juliacommit(results)::String</code> - The commit of the Julia executable that ran the benchmarks</p></li><li><p><code>benchmarkgroup(results)::BenchmarkGroup</code> - a <a href="https://github.com/JuliaCI/BenchmarkTools.jl/blob/master/doc/manual.md#the-benchmarkgroup-type"><code>BenchmarkGroup</code></a>  contaning the results of the benchmark.</p></li><li><p><code>date(results)::DateTime</code> - Tthe time when the benchmarks were executed</p></li><li><p><code>benchmarkconfig(results)::BenchmarkConfig</code> - The <a href="ref.html#PkgBenchmark.BenchmarkConfig"><code>BenchmarkConfig</code></a> used for the benchmarks.</p></li></ul><p><code>BenchmarkResults</code> can be exported to markdown using the function <a href="export_markdown.html#PkgBenchmark.export_markdown"><code>export_markdown</code></a>.</p></div><a class="source-link" target="_blank" href="https://github.com/JuliaCI/PkgBenchmark.jl/blob/d3e7a0de30c5a1346eb827c512fa7e4111fcd825/src/benchmarkresults.jl#L1-L15">source</a></section><h2><a class="nav-anchor" id="More-advanced-customization-1" href="#More-advanced-customization-1">More advanced customization</a></h2><p>Instead of passing a commit, branch etc. as a <code>String</code> to <code>benchmarkpkg</code>, a <a href="ref.html#PkgBenchmark.BenchmarkConfig"><code>BenchmarkConfig</code></a> can be passed. This object contains the package commit, julia command, and what environment variables will be used when benchmarking. The default values can be seen by using the default constructor</p><pre><code class="language-julia-repl">julia&gt; BenchmarkConfig()
BenchmarkConfig:
    id: nothing
    juliacmd: `/home/user/julia/julia`
    env:</code></pre><p>The <code>id</code> is a commit, branch etc as described in the previous section. An <code>id</code> with value <code>nothing</code> means that the current state of the package will be benchmarked. The default value of <code>juliacmd</code> is <code>joinpath(JULIA_HOME, Base.julia_exename()</code> which is the command to run the julia executable without any command line arguments.</p><p>To instead benchmark the branch <code>PR</code>, using the julia command <code>julia -O3</code> with the environment variable <code>JULIA_NUM_THREADS</code> set to <code>4</code>, the config would be created as</p><pre><code class="language-julia-repl">julia&gt; config = BenchmarkConfig(id = &quot;PR&quot;,
                                juliacmd = `julia -O3`,
                                env = Dict(&quot;JULIA_NUM_THREADS&quot; =&gt; 4))
BenchmarkConfig:
    id: PR
    juliacmd: `julia -O3`
    env: JULIA_NUM_THREADS =&gt; 4</code></pre><p>To benchmark the package with the config, call <a href="run_benchmarks.html#PkgBenchmark.benchmarkpkg"><code>benchmarkpkg</code></a> as e.g.</p><pre><code class="language-julia">benchmark(&quot;Tensors&quot;, config)</code></pre><div class="admonition info"><div class="admonition-title">Info</div><div class="admonition-text"><p>The <code>id</code> keyword to the <code>BenchmarkConfig</code> does not have to be a branch, it can be most things that git can understand, for example a commit id or a tag.</p></div></div><footer><hr/><a class="previous" href="define_benchmarks.html"><span class="direction">Previous</span><span class="title">Defining a benchmark suite</span></a><a class="next" href="comparing_commits.html"><span class="direction">Next</span><span class="title">Comparing commits</span></a></footer></article></body></html>
