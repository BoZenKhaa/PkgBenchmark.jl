<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Running a benchmark suite · PkgBenchmark.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/highlightjs/default.css" rel="stylesheet" type="text/css"/><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>PkgBenchmark.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" action="../search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../index.html">Home</a></li><li><span class="toctext">Manual</span><ul><li><a class="toctext" href="define_benchmarks.html">Defining a benchmark suite</a></li><li class="current"><a class="toctext" href="run_benchmarks.html">Running a benchmark suite</a><ul class="internal"><li><a class="toctext" href="#Comparing-commits-1">Comparing commits</a></li></ul></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Manual</li><li><a href="run_benchmarks.html">Running a benchmark suite</a></li></ul><a class="edit-page" href="https://github.com/JuliaCI/PkgBenchmark.jl/tree/46286e3f1e57e51611421368173ce434e66fa4c0/docs/src/man/run_benchmarks.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Running a benchmark suite</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Running-a-benchmark-suite-1" href="#Running-a-benchmark-suite-1">Running a benchmark suite</a></h1><p>Use <code>benchmarkpkg</code> to run benchmarks written using the convention above.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="PkgBenchmark.benchmarkpkg" href="#PkgBenchmark.benchmarkpkg"><code>PkgBenchmark.benchmarkpkg</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">benchmarkpkg(pkg, [ref];
            script=defaultscript(pkg),
            require=defaultrequire(pkg),
            resultsdir=defaultresultsdir(pkg),
            saveresults=true,
            tunefile=defaulttunefile(pkg),
            retune=false,
            promptsave=true,
            promptoverwrite=true)</code></pre><p><strong>Arguments</strong>:</p><ul><li><p><code>pkg</code> is the package to benchmark</p></li><li><p><code>ref</code> is the commit/branch to checkout for benchmarking. If left out, the package will be benchmarked in its current state.</p></li></ul><p><strong>Keyword arguments</strong>:</p><ul><li><p><code>script</code> is the script with the benchmarks. Defaults to <code>PKG/benchmark/benchmarks.jl</code></p></li><li><p><code>require</code> is the REQUIRE file containing dependencies needed for the benchmark. Defaults to <code>PKG/benchmark/REQUIRE</code>.</p></li><li><p><code>resultsdir</code> the directory where to file away results. Defaults to <code>PKG/benchmark/.results</code>. Provided the repository is not dirty, results generated will be saved in this directory in a file named <code>&lt;SHA1_of_commit&gt;.jld</code>. And can be used later by functions such as <code>judge</code>. If you choose to, you can save the results manually using <code>writeresults(file, results)</code> where <code>results</code> is the return value of <code>benchmarkpkg</code> function. It can be read back with <code>readresults(file)</code>.</p></li><li><p><code>saveresults</code> if set to false, results will not be saved in <code>resultsdir</code>.</p></li><li><p><code>promptsave</code> if set to false, you will prompted to confirm before saving the results.</p></li><li><p><code>tunefile</code> file to use for tuning benchmarks, will be created if doesn&#39;t exist. Defaults to <code>PKG/benchmark/.tune.jld</code></p></li><li><p><code>retune</code> force a re-tune, saving results to the tune file</p></li><li><p><code>promptsave</code> if set to false, you will prompted to confirm before saving the results.</p></li><li><p><code>promptoverwrite</code> if set to false, will not asked to confirm before overwriting previously saved results for a commit.</p></li></ul><p><strong>Returns:</strong></p><p>A <code>BenchmarkGroup</code> object with the results of the benchmark.</p><p><strong>Example invocations:</strong></p><pre><code class="language-julia">using PkgBenchmark

benchmarkpkg(&quot;MyPkg&quot;) # run the benchmarks at the current state of the repository
benchmarkpkg(&quot;MyPkg&quot;, &quot;my-feature&quot;) # run the benchmarks for a particular branch/commit/tag
benchmarkpkg(&quot;MyPkg&quot;, &quot;my-feature&quot;; script=&quot;/home/me/mycustombenchmark.jl&quot;, resultsdir=&quot;/home/me/benchmarkXresults&quot;)
  # note: its a good idea to set a new resultsdir with a new benchmark script. `PKG/benchmark/.results` is meant for `PKG/benchmark/benchmarks.jl` script.</code></pre></div><a class="source-link" target="_blank" href="https://github.com/JuliaCI/PkgBenchmark.jl/tree/46286e3f1e57e51611421368173ce434e66fa4c0/src/runbenchmark.jl#L54-L96">source</a><br/></section><h2><a class="nav-anchor" id="Comparing-commits-1" href="#Comparing-commits-1">Comparing commits</a></h2><p>You can use <code>judge</code> to compare benchmark results of two versions of the package.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="BenchmarkTools.judge" href="#BenchmarkTools.judge"><code>BenchmarkTools.judge</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">judge(pkg, [ref], baseline;
    f=(minimum, minimum),
    usesaved=(true, true),
    script=defaultscript(pkg),
    require=defaultrequire(pkg),
    resultsdir=defaultresultsdir(pkg),
    saveresults=true,
    promptsave=true,
    promptoverwrite=true)</code></pre><p>You can call <code>showall(results)</code> to see a comparison of all the benchmarks.</p><p><strong>Arguments</strong>:</p><ul><li><p><code>pkg</code> is the package to benchmark</p></li><li><p><code>ref</code> optional, the commit to judge. If skipped, use the current state of the package repo.</p></li><li><p><code>baseline</code> is the commit to compare <code>ref</code> against.</p></li></ul><p><strong>Keyword arguments</strong>:</p><ul><li><p><code>f</code> - tuple of estimator functions - one each for <code>from_ref</code>, <code>to_ref</code> respectively</p></li><li><p><code>use_saved</code> - similar tuple of flags, if false will not use saved results</p></li><li><p>for description of other keyword arguments, see <code>benchmarkpkg</code></p></li></ul></div><a class="source-link" target="_blank" href="https://github.com/JuliaCI/PkgBenchmark.jl/tree/46286e3f1e57e51611421368173ce434e66fa4c0/src/judge.jl#L27-L51">source</a><br/></section><footer><hr/><a class="previous" href="define_benchmarks.html"><span class="direction">Previous</span><span class="title">Defining a benchmark suite</span></a></footer></article></body></html>
